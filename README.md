# Sistema de An√°lisis SEO de Dominios

Una API completa desarrollada en FastAPI para an√°lisis exhaustivo de dominios web, extracci√≥n de keywords inteligente y clasificaci√≥n de contenido. El sistema descubre autom√°ticamente sitemaps, analiza p√°ginas individuales y genera keywords clasificadas en buckets espec√≠ficos usando algoritmos avanzados de NLP.

## üöÄ Caracter√≠sticas Principales

- **Descubrimiento Autom√°tico de Sitemaps**: Detecta y parsea sitemaps XML, incluyendo sitemap indexes
- **An√°lisis Completo de P√°ginas**: Extrae metadatos, headings, contenido principal y datos de schema.org
- **Clasificaci√≥n Inteligente**: Determina tipo de p√°gina (e-commerce/blog/mixto), audiencia e intenci√≥n
- **Extracci√≥n de Keywords Avanzada**: Combina YAKE y KeyBERT con MiniLM para m√°xima precisi√≥n
- **Bucketizaci√≥n Inteligente**: Clasifica keywords en buckets espec√≠ficos (cliente/producto/generales)
- **Scoring Personalizable**: F√≥rmula ponderada ajustable din√°micamente
- **Procesamiento As√≠ncrono**: An√°lisis paralelo de m√∫ltiples URLs con rate limiting
- **Extracci√≥n de Productos**: Detecta productos autom√°ticamente desde schema.org
- **API REST Completa**: Endpoints documentados con OpenAPI/Swagger

## üèóÔ∏è Arquitectura

```
N8NMC/
‚îú‚îÄ‚îÄ app/                 # Aplicaci√≥n principal FastAPI
‚îÇ   ‚îú‚îÄ‚îÄ main.py         # FastAPI app, endpoints, middleware
‚îÇ   ‚îú‚îÄ‚îÄ config.py       # Configuraci√≥n y pesos ajustables
‚îÇ   ‚îú‚îÄ‚îÄ schemas.py      # Modelos Pydantic request/response
‚îÇ   ‚îî‚îÄ‚îÄ services/       # Servicios del sistema
‚îÇ       ‚îú‚îÄ‚îÄ fetcher.py  # HTTP as√≠ncrono con retries
‚îÇ       ‚îú‚îÄ‚îÄ sitemap.py  # Descubrimiento y parseo sitemaps
‚îÇ       ‚îú‚îÄ‚îÄ parser.py   # Extracci√≥n HTML completa
‚îÇ       ‚îú‚îÄ‚îÄ classifier.py # Clasificaci√≥n blog/ecom/audiencia
‚îÇ       ‚îú‚îÄ‚îÄ nlp.py      # YAKE, KeyBERT, normalizaci√≥n
‚îÇ       ‚îú‚îÄ‚îÄ ecom.py     # Extracci√≥n productos
‚îÇ       ‚îú‚îÄ‚îÄ scorer.py   # Scoring y bucketizaci√≥n
‚îÇ       ‚îî‚îÄ‚îÄ utils.py    # Stopwords, helpers, regex
‚îú‚îÄ‚îÄ tests/              # Pruebas principales del sistema
‚îÇ   ‚îú‚îÄ‚îÄ test_logitech_domain.py  # Prueba an√°lisis de dominio
‚îÇ   ‚îî‚îÄ‚îÄ test_speedlogic.py       # Prueba an√°lisis de URL
‚îú‚îÄ‚îÄ scripts/            # Scripts de utilidad y desarrollo
‚îÇ   ‚îú‚îÄ‚îÄ start.py        # Script de inicio
‚îÇ   ‚îú‚îÄ‚îÄ example_usage.py # Ejemplos de uso
‚îÇ   ‚îî‚îÄ‚îÄ debug_api.py    # Herramientas de debugging
‚îú‚îÄ‚îÄ docs/               # Documentaci√≥n del proyecto
‚îÇ   ‚îú‚îÄ‚îÄ IMPLEMENTATION_SUMMARY.md
‚îÇ   ‚îî‚îÄ‚îÄ cloud-run-deploy.md
‚îú‚îÄ‚îÄ results/            # Resultados de an√°lisis guardados
‚îú‚îÄ‚îÄ archive/            # Archivos antiguos y pruebas obsoletas
‚îú‚îÄ‚îÄ docker-compose.yml  # Configuraci√≥n Docker
‚îú‚îÄ‚îÄ Dockerfile         # Imagen Docker
‚îú‚îÄ‚îÄ requirements.txt   # Dependencias Python
‚îî‚îÄ‚îÄ README.md          # Este archivo
```

## üìã Requisitos

- Python 3.11+
- Docker (opcional)
- Google Cloud SDK (para deploy)

## üõ†Ô∏è Instalaci√≥n

### Desarrollo Local

1. **Clonar el repositorio**
```bash
git clone <repository-url>
cd N8NMC
```

2. **Crear entorno virtual**
```bash
python -m venv venv
source venv/bin/activate  # Linux/Mac
# o
venv\Scripts\activate    # Windows
```

3. **Instalar dependencias**
```bash
pip install -r requirements.txt
```

4. **Configurar variables de entorno**
```bash
cp env.example .env
# Editar .env con tus configuraciones
```

5. **Inicio r√°pido (recomendado)**
```bash
python start.py
```

6. **O ejecutar manualmente**
```bash
uvicorn app.main:app --host 0.0.0.0 --port 8080 --reload
```

### Verificaci√≥n del Sistema

Ejecuta las pruebas principales para verificar que todo funciona:
```bash
# Prueba de an√°lisis de dominio (Logitech)
python tests/test_logitech_domain.py

# Prueba de an√°lisis de URL individual (SpeedLogic)
python tests/test_speedlogic.py
```

Ejecuta el ejemplo de uso:
```bash
python scripts/example_usage.py
```

### Docker

1. **Construir imagen**
```bash
docker build -t seo-analysis-api .
```

2. **Ejecutar con Docker Compose**
```bash
docker-compose up -d
```

3. **Verificar funcionamiento**
```bash
curl http://localhost:8080/healthz
```

## üìö Uso de la API

### Autenticaci√≥n

Todas las requests requieren header de autenticaci√≥n:
```bash
curl -H "X-API-Key: your-secret-api-key-here" ...
```

### Endpoints Principales

#### 1. An√°lisis de URL Individual

```bash
curl -X POST "http://localhost:8080/analyze-url" \
  -H "X-API-Key: your-api-key" \
  -H "Content-Type: application/json" \
  -d '{
    "url": "https://ejemplo.com/producto-zapatillas"
  }'
```

**Response:**
```json
{
  "url": "https://ejemplo.com/producto-zapatillas",
  "tipo": "ecommerce",
  "meta": {
    "title": "Zapatillas Running XYZ - Mejor Precio",
    "description": "Zapatillas de running profesionales...",
    "og_title": "Zapatillas Running XYZ",
    "og_description": "Descubre las mejores zapatillas...",
    "canonical": "https://ejemplo.com/producto-zapatillas",
    "lang": "es"
  },
  "headings": {
    "h1": ["Zapatillas Running XYZ"],
    "h2": ["Caracter√≠sticas", "Especificaciones", "Opiniones"],
    "h3": ["Material", "Suela", "Tallas disponibles"]
  },
  "stats": {
    "words": 1234,
    "reading_time_min": 6,
    "internal_links": 24,
    "external_links": 3
  },
  "audiencia": ["principiantes", "B2C"],
  "intencion": "comercial",
  "productos": [
    {
      "nombre": "Zapatillas Running XYZ",
      "categoria": "Calzado Deportivo",
      "marca": "MarcaA",
      "precio": 89.99,
      "moneda": "EUR"
    }
  ],
  "keywords": {
    "cliente": [
      {"term": "marcaA", "score": 0.83},
      {"term": "tienda online", "score": 0.61}
    ],
    "producto_o_post": [
      {"term": "zapatillas running", "score": 0.78},
      {"term": "calzado deportivo", "score": 0.72}
    ],
    "generales_seo": [
      {"term": "deportes", "score": 0.65},
      {"term": "ejercicio", "score": 0.58}
    ]
  }
}
```

#### 2. An√°lisis Completo de Dominio

```bash
curl -X POST "http://localhost:8080/analyze-domain" \
  -H "X-API-Key: your-api-key" \
  -H "Content-Type: application/json" \
  -d '{
    "domain": "https://ejemplo.com",
    "max_urls": 50,
    "timeout": 15
  }'
```

**Response:**
```json
{
  "domain": "https://ejemplo.com",
  "resumen": {
    "total_urls": 45,
    "por_tipo": {
      "ecommerce": 28,
      "blog": 15,
      "mixto": 2
    },
    "top_keywords_cliente": [
      {"term": "marcaA", "score": 0.89},
      {"term": "tienda online", "score": 0.76}
    ],
    "top_keywords_producto": [
      {"term": "zapatillas", "score": 0.82},
      {"term": "ropa deportiva", "score": 0.71}
    ],
    "top_keywords_generales": [
      {"term": "deportes", "score": 0.68},
      {"term": "salud", "score": 0.62}
    ]
  },
  "urls": [
    // Array de an√°lisis individuales por URL
  ]
}
```

#### 3. Gesti√≥n de Pesos de Scoring

**Obtener pesos actuales:**
```bash
curl -X GET "http://localhost:8080/scoring-weights" \
  -H "X-API-Key: your-api-key"
```

**Actualizar pesos:**
```bash
curl -X PUT "http://localhost:8080/scoring-weights" \
  -H "X-API-Key: your-api-key" \
  -H "Content-Type: application/json" \
  -d '{
    "w1_frequency": 0.4,
    "w2_tfidf": 0.3,
    "w3_cooccurrence": 0.2,
    "w4_position_title": 0.05,
    "w5_similarity_brand": 0.05
  }'
```

#### 4. Health Check

```bash
curl -X GET "http://localhost:8080/healthz"
```

## ‚öôÔ∏è Configuraci√≥n

### Variables de Entorno

| Variable | Descripci√≥n | Valor por Defecto |
|----------|-------------|-------------------|
| `API_KEY` | Clave API para autenticaci√≥n | `your-secret-api-key-here` |
| `MAX_CONCURRENT_REQUESTS` | Requests concurrentes m√°ximos | `10` |
| `DEFAULT_TIMEOUT` | Timeout por request (segundos) | `15` |
| `MAX_URLS_PER_DOMAIN` | URLs m√°ximas por dominio | `100` |
| `W1_FREQUENCY` | Peso frecuencia en scoring | `0.3` |
| `W2_TFIDF` | Peso TF-IDF en scoring | `0.25` |
| `W3_COOCCURRENCE` | Peso co-ocurrencias | `0.2` |
| `W4_POSITION_TITLE` | Peso posici√≥n en t√≠tulo | `0.15` |
| `W5_SIMILARITY_BRAND` | Peso similitud con marca | `0.1` |

### F√≥rmula de Scoring

El score final de cada keyword se calcula usando la f√≥rmula:

```
score = w1*freq + w2*tfidf + w3*cooccur + w4*pos_title + w5*sim_brand
```

Donde:
- **freq**: Frecuencia normalizada del t√©rmino
- **tfidf**: Score TF-IDF del t√©rmino
- **cooccur**: Co-ocurrencias en headings importantes
- **pos_title**: Posici√≥n en el t√≠tulo (m√°s alto al inicio)
- **sim_brand**: Similitud con la marca detectada

## üöÄ Deploy en Google Cloud Run

### 1. Configuraci√≥n Inicial

```bash
# Autenticarse en Google Cloud
gcloud auth login
gcloud config set project YOUR_PROJECT_ID
gcloud config set run/region europe-west1
```

### 2. Build y Deploy

```bash
# Construir imagen
gcloud builds submit --tag gcr.io/YOUR_PROJECT_ID/seo-analysis-api

# Deploy a Cloud Run
gcloud run deploy seo-analysis-api \
  --image gcr.io/YOUR_PROJECT_ID/seo-analysis-api \
  --platform managed \
  --region europe-west1 \
  --allow-unauthenticated \
  --memory 2Gi \
  --cpu 1 \
  --timeout 300 \
  --concurrency 10 \
  --max-instances 10 \
  --port 8080 \
  --set-env-vars "API_KEY=your-production-api-key"
```

### 3. Configuraci√≥n Recomendada para Producci√≥n

- **Memory**: 2GB (necesario para modelos NLP)
- **CPU**: 1 vCPU
- **Timeout**: 300s (an√°lisis de dominio puede ser largo)
- **Concurrency**: 10-20 requests simult√°neos
- **Max instances**: 10-50 dependiendo del tr√°fico
- **Min instances**: 0 para ahorrar costos

## üìä Buckets de Keywords

El sistema clasifica autom√°ticamente las keywords en tres buckets:

### 1. **Cliente** (`keywords_cliente`)
- Marca y t√©rminos del dominio
- Keywords con alta frecuencia global
- T√©rminos que aparecen en m√∫ltiples p√°ginas

### 2. **Producto/Post** (`producto_o_post`)
- Keywords espec√≠ficas de la p√°gina actual
- T√©rminos relacionados con productos (e-commerce)
- Contenido espec√≠fico del post (blog)

### 3. **Generales SEO** (`generales_seo`)
- T√©rminos amplios y head terms
- Keywords de intenci√≥n informacional
- Entidades gen√©ricas del dominio

## üîç Algoritmos de NLP

### YAKE (Yet Another Keyword Extractor)
- Extracci√≥n r√°pida sin embeddings
- Optimizado para espa√±ol
- N-gramas 1-2 configurable

### KeyBERT + MiniLM
- Modelo `all-MiniLM-L6-v2`
- Extracci√≥n sem√°ntica avanzada
- Deduplicaci√≥n por similitud coseno

### Fusi√≥n Inteligente
- Combina resultados de ambos algoritmos
- Elimina duplicados por similitud > 0.85
- Normaliza scores finales

## üõ°Ô∏è Seguridad

- **Autenticaci√≥n**: Header `X-API-Key` requerido
- **Rate Limiting**: Delay configurable entre requests
- **Robots.txt**: Respeta restricciones autom√°ticamente
- **User-Agent**: Headers realistas para evitar bloqueos
- **Timeouts**: L√≠mites configurables por request

## üìà Monitoreo y Logging

- **Logs estructurados**: Formato JSON con timestamps
- **Health checks**: Endpoint `/healthz` para monitoreo
- **M√©tricas**: Tiempo de procesamiento y estad√≠sticas
- **Error handling**: Manejo robusto de excepciones

## üêõ Troubleshooting

### Problemas Comunes

1. **Error de memoria**: Aumentar memoria a 2GB m√≠nimo
2. **Timeout en an√°lisis de dominio**: Aumentar `DEFAULT_TIMEOUT`
3. **Rate limiting**: Ajustar `MAX_CONCURRENT_REQUESTS`
4. **Modelos no cargan**: Verificar conexi√≥n a internet para descarga inicial

### Logs √ötiles

```bash
# Ver logs en tiempo real
docker-compose logs -f api

# Ver logs espec√≠ficos
grep "ERROR" logs/app.log
```

## ü§ù Contribuci√≥n

1. Fork el proyecto
2. Crear branch para feature (`git checkout -b feature/nueva-funcionalidad`)
3. Commit cambios (`git commit -am 'A√±adir nueva funcionalidad'`)
4. Push al branch (`git push origin feature/nueva-funcionalidad`)
5. Crear Pull Request

## üìÑ Licencia

Este proyecto est√° bajo la Licencia MIT. Ver `LICENSE` para m√°s detalles.

## üìû Soporte

Para soporte t√©cnico o preguntas:
- Crear issue en GitHub
- Email: soporte@ejemplo.com
- Documentaci√≥n: `/docs` (Swagger UI)

---

**Versi√≥n**: 1.0.0  
**√öltima actualizaci√≥n**: Enero 2025
