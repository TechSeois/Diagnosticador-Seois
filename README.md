# Sistema de AnÃ¡lisis SEO de Dominios

Una API completa desarrollada en FastAPI para anÃ¡lisis exhaustivo de dominios web, extracciÃ³n de keywords inteligente y clasificaciÃ³n de contenido. El sistema descubre automÃ¡ticamente sitemaps, analiza pÃ¡ginas individuales y genera keywords clasificadas en buckets especÃ­ficos usando algoritmos avanzados de NLP.

## ğŸš€ CaracterÃ­sticas Principales

- **Descubrimiento AutomÃ¡tico de Sitemaps**: Detecta y parsea sitemaps XML, incluyendo sitemap indexes
- **AnÃ¡lisis Completo de PÃ¡ginas**: Extrae metadatos, headings, contenido principal y datos de schema.org
- **ClasificaciÃ³n Inteligente**: Determina tipo de pÃ¡gina (e-commerce/blog/mixto), audiencia e intenciÃ³n
- **ExtracciÃ³n de Keywords Avanzada**: Combina YAKE y KeyBERT con MiniLM para mÃ¡xima precisiÃ³n
- **BucketizaciÃ³n Inteligente**: Clasifica keywords en buckets especÃ­ficos (cliente/producto/generales)
- **Scoring Personalizable**: FÃ³rmula ponderada ajustable dinÃ¡micamente
- **Procesamiento AsÃ­ncrono**: AnÃ¡lisis paralelo de mÃºltiples URLs con rate limiting
- **ExtracciÃ³n de Productos**: Detecta productos automÃ¡ticamente desde schema.org
- **API REST Completa**: Endpoints documentados con OpenAPI/Swagger

## ğŸ—ï¸ Arquitectura

```
N8NMC/
â”œâ”€â”€ app/                 # AplicaciÃ³n principal FastAPI
â”‚   â”œâ”€â”€ main.py         # FastAPI app, endpoints, middleware
â”‚   â”œâ”€â”€ config.py       # ConfiguraciÃ³n y pesos ajustables
â”‚   â”œâ”€â”€ schemas.py      # Modelos Pydantic request/response
â”‚   â””â”€â”€ services/       # Servicios del sistema
â”‚       â”œâ”€â”€ fetcher.py  # HTTP asÃ­ncrono con retries
â”‚       â”œâ”€â”€ sitemap.py  # Descubrimiento y parseo sitemaps
â”‚       â”œâ”€â”€ parser.py   # ExtracciÃ³n HTML completa
â”‚       â”œâ”€â”€ classifier.py # ClasificaciÃ³n blog/ecom/audiencia
â”‚       â”œâ”€â”€ nlp.py      # YAKE, KeyBERT, normalizaciÃ³n
â”‚       â”œâ”€â”€ ecom.py     # ExtracciÃ³n productos
â”‚       â”œâ”€â”€ scorer.py   # Scoring y bucketizaciÃ³n
â”‚       â””â”€â”€ utils.py    # Stopwords, helpers, regex
â”œâ”€â”€ tests/              # Pruebas principales del sistema
â”‚   â”œâ”€â”€ test_logitech_domain.py  # Prueba anÃ¡lisis de dominio
â”‚   â””â”€â”€ test_speedlogic.py       # Prueba anÃ¡lisis de URL
â”œâ”€â”€ scripts/            # Scripts de utilidad y desarrollo
â”‚   â”œâ”€â”€ start.py        # Script de inicio
â”‚   â”œâ”€â”€ example_usage.py # Ejemplos de uso
â”‚   â””â”€â”€ debug_api.py    # Herramientas de debugging
â”œâ”€â”€ docs/               # DocumentaciÃ³n del proyecto
â”‚   â”œâ”€â”€ IMPLEMENTATION_SUMMARY.md
â”‚   â””â”€â”€ cloud-run-deploy.md
â”œâ”€â”€ results/            # Resultados de anÃ¡lisis guardados
â”œâ”€â”€ archive/            # Archivos antiguos y pruebas obsoletas
â”œâ”€â”€ docker-compose.yml  # ConfiguraciÃ³n Docker
â”œâ”€â”€ Dockerfile         # Imagen Docker
â”œâ”€â”€ requirements.txt   # Dependencias Python
â””â”€â”€ README.md          # Este archivo
```

## ğŸ“‹ Requisitos

- Python 3.11+
- Docker (opcional)
- Google Cloud SDK (para deploy)

## ğŸ› ï¸ InstalaciÃ³n

### Desarrollo Local

1. **Clonar el repositorio**
```bash
git clone <repository-url>
cd N8NMC
```

2. **Crear entorno virtual**
```bash
python -m venv venv
source venv/bin/activate  # Linux/Mac
# o
venv\Scripts\activate    # Windows
```

3. **Instalar dependencias**
```bash
pip install -r requirements.txt
```

4. **Configurar variables de entorno**
```bash
cp env.example .env
# Editar .env con tus configuraciones
```

5. **Inicio rÃ¡pido (recomendado)**
```bash
python start.py
```

6. **O ejecutar manualmente**
```bash
uvicorn app.main:app --host 0.0.0.0 --port 8080 --reload
```

### VerificaciÃ³n del Sistema

Ejecuta las pruebas principales para verificar que todo funciona:
```bash
# Prueba de anÃ¡lisis de dominio (Logitech)
python tests/test_logitech_domain.py

# Prueba de anÃ¡lisis de URL individual (SpeedLogic)
python tests/test_speedlogic.py
```

Ejecuta el ejemplo de uso:
```bash
python scripts/example_usage.py
```

### Docker

1. **Construir imagen**
```bash
docker build -t seo-analysis-api .
```

2. **Ejecutar con Docker Compose**
```bash
docker-compose up -d
```

3. **Verificar funcionamiento**
```bash
curl http://localhost:8080/healthz
```

## ğŸ“š Uso de la API

### AutenticaciÃ³n

Todas las requests requieren header de autenticaciÃ³n:
```bash
curl -H "X-API-Key: your-secret-api-key-here" ...
```

### Endpoints Principales

#### 1. AnÃ¡lisis de URL Individual

```bash
curl -X POST "http://localhost:8080/analyze-url" \
  -H "X-API-Key: your-api-key" \
  -H "Content-Type: application/json" \
  -d '{
    "url": "https://ejemplo.com/producto-zapatillas"
  }'
```

**Response:**
```json
{
  "url": "https://ejemplo.com/producto-zapatillas",
  "tipo": "ecommerce",
  "meta": {
    "title": "Zapatillas Running XYZ - Mejor Precio",
    "description": "Zapatillas de running profesionales...",
    "og_title": "Zapatillas Running XYZ",
    "og_description": "Descubre las mejores zapatillas...",
    "canonical": "https://ejemplo.com/producto-zapatillas",
    "lang": "es"
  },
  "headings": {
    "h1": ["Zapatillas Running XYZ"],
    "h2": ["CaracterÃ­sticas", "Especificaciones", "Opiniones"],
    "h3": ["Material", "Suela", "Tallas disponibles"]
  },
  "stats": {
    "words": 1234,
    "reading_time_min": 6,
    "internal_links": 24,
    "external_links": 3
  },
  "audiencia": ["principiantes", "B2C"],
  "intencion": "comercial",
  "productos": [
    {
      "nombre": "Zapatillas Running XYZ",
      "categoria": "Calzado Deportivo",
      "marca": "MarcaA",
      "precio": 89.99,
      "moneda": "EUR"
    }
  ],
  "keywords": {
    "cliente": [
      {"term": "marcaA", "score": 0.83},
      {"term": "tienda online", "score": 0.61}
    ],
    "producto_o_post": [
      {"term": "zapatillas running", "score": 0.78},
      {"term": "calzado deportivo", "score": 0.72}
    ],
    "generales_seo": [
      {"term": "deportes", "score": 0.65},
      {"term": "ejercicio", "score": 0.58}
    ]
  }
}
```

#### 2. AnÃ¡lisis Completo de Dominio

```bash
curl -X POST "http://localhost:8080/analyze-domain" \
  -H "X-API-Key: your-api-key" \
  -H "Content-Type: application/json" \
  -d '{
    "domain": "https://ejemplo.com",
    "max_urls": 50,
    "timeout": 15
  }'
```

**Response:**
```json
{
  "domain": "https://ejemplo.com",
  "resumen": {
    "total_urls": 45,
    "por_tipo": {
      "ecommerce": 28,
      "blog": 15,
      "mixto": 2
    },
    "top_keywords_cliente": [
      {"term": "marcaA", "score": 0.89},
      {"term": "tienda online", "score": 0.76}
    ],
    "top_keywords_producto": [
      {"term": "zapatillas", "score": 0.82},
      {"term": "ropa deportiva", "score": 0.71}
    ],
    "top_keywords_generales": [
      {"term": "deportes", "score": 0.68},
      {"term": "salud", "score": 0.62}
    ]
  },
  "urls": [
    // Array de anÃ¡lisis individuales por URL
  ]
}
```

#### 3. GestiÃ³n de Pesos de Scoring

**Obtener pesos actuales:**
```bash
curl -X GET "http://localhost:8080/scoring-weights" \
  -H "X-API-Key: your-api-key"
```

**Actualizar pesos:**
```bash
curl -X PUT "http://localhost:8080/scoring-weights" \
  -H "X-API-Key: your-api-key" \
  -H "Content-Type: application/json" \
  -d '{
    "w1_frequency": 0.4,
    "w2_tfidf": 0.3,
    "w3_cooccurrence": 0.2,
    "w4_position_title": 0.05,
    "w5_similarity_brand": 0.05
  }'
```

#### 4. Health Check

```bash
curl -X GET "http://localhost:8080/healthz"
```

## âš™ï¸ ConfiguraciÃ³n

### Variables de Entorno

| Variable | DescripciÃ³n | Valor por Defecto |
|----------|-------------|-------------------|
| `API_KEY` | Clave API para autenticaciÃ³n | `your-secret-api-key-here` |
| `MAX_CONCURRENT_REQUESTS` | Requests concurrentes mÃ¡ximos | `10` |
| `DEFAULT_TIMEOUT` | Timeout por request (segundos) | `15` |
| `MAX_URLS_PER_DOMAIN` | URLs mÃ¡ximas por dominio | `100` |
| `W1_FREQUENCY` | Peso frecuencia en scoring | `0.3` |
| `W2_TFIDF` | Peso TF-IDF en scoring | `0.25` |
| `W3_COOCCURRENCE` | Peso co-ocurrencias | `0.2` |
| `W4_POSITION_TITLE` | Peso posiciÃ³n en tÃ­tulo | `0.15` |
| `W5_SIMILARITY_BRAND` | Peso similitud con marca | `0.1` |

### FÃ³rmula de Scoring

El score final de cada keyword se calcula usando la fÃ³rmula:

```
score = w1*freq + w2*tfidf + w3*cooccur + w4*pos_title + w5*sim_brand
```

Donde:
- **freq**: Frecuencia normalizada del tÃ©rmino
- **tfidf**: Score TF-IDF del tÃ©rmino
- **cooccur**: Co-ocurrencias en headings importantes
- **pos_title**: PosiciÃ³n en el tÃ­tulo (mÃ¡s alto al inicio)
- **sim_brand**: Similitud con la marca detectada

## ğŸš€ Deploy en Google Cloud Run

### 1. ConfiguraciÃ³n Inicial

```bash
# Autenticarse en Google Cloud
gcloud auth login
gcloud config set project YOUR_PROJECT_ID
gcloud config set run/region europe-west1
```

### 2. Build y Deploy

```bash
# Construir imagen
gcloud builds submit --tag gcr.io/YOUR_PROJECT_ID/seo-analysis-api

# Deploy a Cloud Run
gcloud run deploy seo-analysis-api \
  --image gcr.io/YOUR_PROJECT_ID/seo-analysis-api \
  --platform managed \
  --region europe-west1 \
  --allow-unauthenticated \
  --memory 2Gi \
  --cpu 1 \
  --timeout 300 \
  --concurrency 10 \
  --max-instances 10 \
  --port 8080 \
  --set-env-vars "API_KEY=your-production-api-key"
```

### 3. ConfiguraciÃ³n Recomendada para ProducciÃ³n

- **Memory**: 2GB (necesario para modelos NLP)
- **CPU**: 1 vCPU
- **Timeout**: 300s (anÃ¡lisis de dominio puede ser largo)
- **Concurrency**: 10-20 requests simultÃ¡neos
- **Max instances**: 10-50 dependiendo del trÃ¡fico
- **Min instances**: 0 para ahorrar costos

## ğŸ“Š Buckets de Keywords

El sistema clasifica automÃ¡ticamente las keywords en tres buckets:

### 1. **Cliente** (`keywords_cliente`)
- Marca y tÃ©rminos del dominio
- Keywords con alta frecuencia global
- TÃ©rminos que aparecen en mÃºltiples pÃ¡ginas

### 2. **Producto/Post** (`producto_o_post`)
- Keywords especÃ­ficas de la pÃ¡gina actual
- TÃ©rminos relacionados con productos (e-commerce)
- Contenido especÃ­fico del post (blog)

### 3. **Generales SEO** (`generales_seo`)
- TÃ©rminos amplios y head terms
- Keywords de intenciÃ³n informacional
- Entidades genÃ©ricas del dominio

## ğŸ” Algoritmos de NLP

### YAKE (Yet Another Keyword Extractor)
- ExtracciÃ³n rÃ¡pida sin embeddings
- Optimizado para espaÃ±ol
- N-gramas 1-2 configurable

### KeyBERT + MiniLM
- Modelo `all-MiniLM-L6-v2`
- ExtracciÃ³n semÃ¡ntica avanzada
- DeduplicaciÃ³n por similitud coseno

### FusiÃ³n Inteligente
- Combina resultados de ambos algoritmos
- Elimina duplicados por similitud > 0.85
- Normaliza scores finales

## ğŸ›¡ï¸ Seguridad

- **AutenticaciÃ³n**: Header `X-API-Key` requerido
- **Rate Limiting**: Delay configurable entre requests
- **Robots.txt**: Respeta restricciones automÃ¡ticamente
- **User-Agent**: Headers realistas para evitar bloqueos
- **Timeouts**: LÃ­mites configurables por request

## ğŸ“ˆ Monitoreo y Logging

- **Logs estructurados**: Formato JSON con timestamps
- **Health checks**: Endpoint `/healthz` para monitoreo
- **MÃ©tricas**: Tiempo de procesamiento y estadÃ­sticas
- **Error handling**: Manejo robusto de excepciones

## ğŸ› Troubleshooting

### Problemas Comunes

1. **Error de memoria**: Aumentar memoria a 2GB mÃ­nimo
2. **Timeout en anÃ¡lisis de dominio**: Aumentar `DEFAULT_TIMEOUT`
3. **Rate limiting**: Ajustar `MAX_CONCURRENT_REQUESTS`
4. **Modelos no cargan**: Verificar conexiÃ³n a internet para descarga inicial

### Logs Ãštiles

```bash
# Ver logs en tiempo real
docker-compose logs -f api

# Ver logs especÃ­ficos
grep "ERROR" logs/app.log
```

## ğŸ¤ ContribuciÃ³n

1. Fork el proyecto
2. Crear branch para feature (`git checkout -b feature/nueva-funcionalidad`)
3. Commit cambios (`git commit -am 'AÃ±adir nueva funcionalidad'`)
4. Push al branch (`git push origin feature/nueva-funcionalidad`)
5. Crear Pull Request

## ğŸ“„ Licencia

Este proyecto estÃ¡ bajo la Licencia MIT. Ver `LICENSE` para mÃ¡s detalles.

## ğŸ“ Soporte

Para soporte tÃ©cnico o preguntas:
- Crear issue en GitHub
- Email: soporte@ejemplo.com
- DocumentaciÃ³n: `/docs` (Swagger UI)

---

**VersiÃ³n**: 1.0.0  
**Ãšltima actualizaciÃ³n**: Enero 2025
