# üöÄ Sistema de An√°lisis SEO de Dominios - Gu√≠a Completa

**Versi√≥n:** 1.0.0  
**√öltima actualizaci√≥n:** 8 de Octubre 2025  
**Estado:** ‚úÖ Producci√≥n Ready

---

## üìã √çndice

1. [Descripci√≥n del Proyecto](#descripci√≥n-del-proyecto)
2. [Arquitectura](#arquitectura)
3. [Requisitos](#requisitos)
4. [Instalaci√≥n Local](#instalaci√≥n-local)
5. [Levantar el Backend](#levantar-el-backend)
6. [Endpoints de la API](#endpoints-de-la-api)
7. [Deployment en Railway](#deployment-en-railway)
8. [Deployment en Cloud Run](#deployment-en-cloud-run)
9. [Configuraci√≥n de Variables](#configuraci√≥n-de-variables)
10. [Correcciones Docker Aplicadas](#correcciones-docker-aplicadas)
11. [Troubleshooting](#troubleshooting)

---

## üìñ Descripci√≥n del Proyecto

API completa en FastAPI para an√°lisis exhaustivo de dominios web con:
- ‚úÖ Extracci√≥n inteligente de keywords usando YAKE + KeyBERT
- ‚úÖ Clasificaci√≥n autom√°tica de p√°ginas (e-commerce/blog/mixto)
- ‚úÖ An√°lisis de audiencia e intenci√≥n de b√∫squeda
- ‚úÖ Descubrimiento autom√°tico de sitemaps
- ‚úÖ Procesamiento as√≠ncrono y paralelo
- ‚úÖ Bucketizaci√≥n de keywords en 3 categor√≠as
- ‚úÖ Scoring personalizable con f√≥rmula ponderada

---

## üèóÔ∏è Arquitectura

```
N8NMC/
‚îú‚îÄ‚îÄ app/
‚îÇ   ‚îú‚îÄ‚îÄ main.py              # FastAPI app, endpoints, middleware
‚îÇ   ‚îú‚îÄ‚îÄ config.py            # Configuraci√≥n y pesos ajustables
‚îÇ   ‚îú‚îÄ‚îÄ schemas.py           # Modelos Pydantic
‚îÇ   ‚îî‚îÄ‚îÄ services/
‚îÇ       ‚îú‚îÄ‚îÄ fetcher.py       # HTTP as√≠ncrono con retries
‚îÇ       ‚îú‚îÄ‚îÄ sitemap.py       # Descubrimiento y parseo sitemaps
‚îÇ       ‚îú‚îÄ‚îÄ parser.py        # Extracci√≥n HTML completa
‚îÇ       ‚îú‚îÄ‚îÄ classifier.py    # Clasificaci√≥n de p√°ginas
‚îÇ       ‚îú‚îÄ‚îÄ nlp.py           # YAKE, KeyBERT, MiniLM
‚îÇ       ‚îú‚îÄ‚îÄ ecom.py          # Extracci√≥n de productos
‚îÇ       ‚îú‚îÄ‚îÄ scorer.py        # Scoring y bucketizaci√≥n
‚îÇ       ‚îî‚îÄ‚îÄ utils.py         # Stopwords, helpers
‚îú‚îÄ‚îÄ Dockerfile               # Configuraci√≥n Docker optimizada
‚îú‚îÄ‚îÄ docker-compose.yml       # Orquestaci√≥n local
‚îú‚îÄ‚îÄ requirements.txt         # Dependencias Python
‚îú‚îÄ‚îÄ railway.json             # Configuraci√≥n Railway
‚îú‚îÄ‚îÄ nixpacks.toml           # Build config Railway
‚îî‚îÄ‚îÄ Procfile                # Comando de inicio Railway
```

---

## üìã Requisitos

- **Python:** 3.11+
- **RAM:** 2GB m√≠nimo (para modelos NLP)
- **Dependencias principales:**
  - FastAPI 0.109.0
  - Uvicorn 0.27.0
  - HTTPX 0.26.0
  - SentenceTransformers 2.3.1
  - KeyBERT 0.8.4
  - YAKE 0.4.8

---

## üõ†Ô∏è Instalaci√≥n Local

### **1. Clonar el Repositorio**

```bash
git clone https://github.com/TechSeois/Diagnosticador-Seois.git
cd Diagnosticador-Seois
```

### **2. Crear Entorno Virtual**

```bash
# Windows
python -m venv venv
.\venv\Scripts\activate

# Linux/Mac
python3 -m venv venv
source venv/bin/activate
```

### **3. Instalar Dependencias**

```bash
pip install -r requirements.txt
```

### **4. Configurar Variables de Entorno (Opcional)**

```bash
# Copiar ejemplo
cp env.example .env

# Editar .env con tus valores
API_KEY=tu-clave-secreta-aqui
MAX_CONCURRENT_REQUESTS=10
DEFAULT_TIMEOUT=15
```

---

## üöÄ Levantar el Backend

### **Opci√≥n 1: Comando B√°sico**

```bash
uvicorn app.main:app --host 0.0.0.0 --port 8080
```

### **Opci√≥n 2: Con Hot Reload (Desarrollo)**

```bash
uvicorn app.main:app --host 0.0.0.0 --port 8080 --reload
```

### **Opci√≥n 3: Con Workers (Producci√≥n)**

```bash
uvicorn app.main:app --host 0.0.0.0 --port 8080 --workers 4
```

### **Opci√≥n 4: Docker Local**

```bash
# Build
docker build -t seo-analysis-api .

# Run
docker run -p 8080:8080 -e API_KEY=tu-clave seo-analysis-api

# O con docker-compose
docker-compose up -d
```

### **Verificar que Funciona**

```bash
# Health check
curl http://localhost:8080/healthz

# Documentaci√≥n interactiva
# Abrir en navegador: http://localhost:8080/docs
```

---

## üì° Endpoints de la API

### **1. POST /analyze-url**

Analiza una URL individual y extrae keywords clasificadas.

**Request:**
```bash
curl -X POST "http://localhost:8080/analyze-url" \
  -H "X-API-Key: your-secret-api-key-here" \
  -H "Content-Type: application/json" \
  -d '{
    "url": "https://speedlogic.com.co/"
  }'
```

**Response:**
```json
{
  "url": "https://speedlogic.com.co/",
  "tipo": "mixto",
  "meta": {
    "title": "Speed Logic - Tienda de Tecnolog√≠a",
    "description": "..."
  },
  "headings": {
    "h1": ["Speed Logic"],
    "h2": ["Productos", "Ofertas"]
  },
  "stats": {
    "words": 484,
    "reading_time_min": 2
  },
  "audiencia": ["gaming", "professionals"],
  "intencion": "comercial",
  "productos": [...],
  "keywords": {
    "cliente": [],
    "producto_o_post": [
      {"term": "ram", "score": 0.343},
      {"term": "tarjeta gr√°fica", "score": 0.338}
    ],
    "generales_seo": []
  }
}
```

### **2. POST /analyze-domain**

Analiza un dominio completo con m√∫ltiples URLs.

**Request:**
```bash
curl -X POST "http://localhost:8080/analyze-domain" \
  -H "X-API-Key: your-secret-api-key-here" \
  -H "Content-Type: application/json" \
  -d '{
    "domain": "https://speedlogic.com.co",
    "max_urls": 10,
    "timeout": 15
  }'
```

**Response:**
```json
{
  "domain": "https://speedlogic.com.co",
  "resumen": {
    "total_urls": 5,
    "por_tipo": {
      "ecommerce": 0,
      "blog": 1,
      "mixto": 4
    },
    "top_keywords_cliente": [...],
    "top_keywords_producto": [
      {"term": "speed logic", "score": 0.654},
      {"term": "tarjeta", "score": 0.482}
    ],
    "top_keywords_generales": [...]
  },
  "urls": [
    /* Array con an√°lisis detallado de cada URL */
  ]
}
```

### **3. GET /scoring-weights**

Obtiene los pesos actuales de la f√≥rmula de scoring.

```bash
curl -X GET "http://localhost:8080/scoring-weights" \
  -H "X-API-Key: your-secret-api-key-here"
```

### **4. PUT /scoring-weights**

Actualiza los pesos de scoring din√°micamente.

```bash
curl -X PUT "http://localhost:8080/scoring-weights" \
  -H "X-API-Key: your-secret-api-key-here" \
  -H "Content-Type: application/json" \
  -d '{
    "w1_frequency": 0.35,
    "w2_tfidf": 0.30,
    "w3_cooccurrence": 0.20,
    "w4_position_title": 0.10,
    "w5_similarity_brand": 0.05
  }'
```

### **5. GET /healthz**

Health check (no requiere autenticaci√≥n).

```bash
curl http://localhost:8080/healthz
```

---

## üöÇ Deployment en Railway

### **Configuraci√≥n**

Railway detecta autom√°ticamente aplicaciones FastAPI usando los archivos:
- `railway.json` - Configuraci√≥n principal
- `Procfile` - Comando de inicio
- `nixpacks.toml` - Optimizaci√≥n del build

### **Pasos de Deployment**

**1. Subir c√≥digo a GitHub:**
```bash
git push origin master
```

**2. Crear proyecto en Railway:**
- Ve a https://railway.app/dashboard
- Clic en "New Project"
- Selecciona "Deploy from GitHub repo"
- Selecciona tu repositorio

**3. Configurar Variables de Entorno:**

En Railway Dashboard ‚Üí Variables:
```env
API_KEY=tu-clave-secreta-aqui
PORT=8080
MAX_CONCURRENT_REQUESTS=10
DEFAULT_TIMEOUT=15
MAX_URLS_PER_DOMAIN=100
```

**4. Deploy Autom√°tico:**

Railway desplegar√° autom√°ticamente. El proceso toma ~3-5 minutos.

### **URL del Servicio**

Railway te dar√° una URL como:
```
https://diagnosticador-seois-production.up.railway.app
```

### **Verificar Deployment**

```bash
curl https://tu-app.railway.app/healthz
```

---

## ‚òÅÔ∏è Deployment en Cloud Run (Google Cloud)

### **M√©todo 1: Deploy desde C√≥digo Local**

```bash
# 1. Autenticarse
gcloud auth login
gcloud config set project YOUR_PROJECT_ID

# 2. Deploy directo
gcloud run deploy diagnosticador-seois \
  --source . \
  --platform managed \
  --region us-east1 \
  --allow-unauthenticated \
  --memory 2Gi \
  --cpu 1 \
  --timeout 300 \
  --max-instances 10 \
  --set-env-vars "API_KEY=your-secret-api-key-here"
```

### **M√©todo 2: Deploy desde Cloud Build (Autom√°tico)**

**1. Conectar repositorio a Cloud Build:**
- Ve a: https://console.cloud.google.com/cloud-build/triggers
- Clic en "Connect Repository"
- Selecciona tu repositorio GitHub

**2. Crear Trigger:**
- Branch pattern: `^master$`
- Build configuration: `Dockerfile`
- Location: `/Dockerfile`

**3. Push al repositorio:**
```bash
git push origin master
```

Cloud Build desplegar√° autom√°ticamente.

### **URL del Servicio**

```
https://diagnosticador-seois-HASH.us-east1.run.app
```

### **Verificar Deployment**

```bash
curl https://tu-servicio.run.app/healthz
```

---

## ‚öôÔ∏è Configuraci√≥n de Variables

### **Variables de Entorno Principales**

| Variable | Descripci√≥n | Default | Obligatoria |
|----------|-------------|---------|-------------|
| `API_KEY` | Clave de autenticaci√≥n | `your-secret-api-key-here` | ‚úÖ S√≠ |
| `PORT` | Puerto del servidor | `8080` | No (Railway/Cloud Run lo asignan) |
| `MAX_CONCURRENT_REQUESTS` | Requests paralelos | `10` | No |
| `DEFAULT_TIMEOUT` | Timeout en segundos | `15` | No |
| `MAX_URLS_PER_DOMAIN` | URLs m√°ximas por dominio | `100` | No |

### **Pesos de Scoring (Opcionales)**

| Variable | Descripci√≥n | Default |
|----------|-------------|---------|
| `W1_FREQUENCY` | Peso frecuencia | `0.3` |
| `W2_TFIDF` | Peso TF-IDF | `0.25` |
| `W3_COOCCURRENCE` | Peso co-ocurrencias | `0.2` |
| `W4_POSITION_TITLE` | Peso posici√≥n en t√≠tulo | `0.15` |
| `W5_SIMILARITY_BRAND` | Peso similitud con marca | `0.1` |

**F√≥rmula de Scoring:**
```
score = w1*frequency + w2*tfidf + w3*cooccurrence + w4*position_title + w5*similarity_brand
```

### **Variables Docker/NLP (Ya configuradas en Dockerfile)**

| Variable | Prop√≥sito | Valor |
|----------|-----------|-------|
| `TRANSFORMERS_CACHE` | Cach√© Transformers | `/usr/local/share/transformers_cache` |
| `HF_HOME` | Home HuggingFace | `/usr/local/share/huggingface` |
| `SENTENCE_TRANSFORMERS_HOME` | Cach√© SentenceTransformers | `/usr/local/share/sentence_transformers` |
| `NLTK_DATA` | Datos NLTK | `/usr/local/share/nltk_data` |

---

## üê≥ Correcciones Docker Aplicadas

### **Problema Original**

Al ejecutar en Docker/Cloud Run, la aplicaci√≥n fallaba con:
```
PermissionError: [Errno 13] Permission denied: '/home/appuser'
```

**Causa:** Los modelos NLP (NLTK, HuggingFace, SentenceTransformers) intentaban descargar datos en runtime en `/home/appuser`, pero el usuario `appuser` no ten√≠a permisos de escritura.

### **Soluciones Implementadas**

#### **1. Dockerfile - Configuraci√≥n de Cach√©s (L√≠neas 31-47)**

```dockerfile
# Configurar directorios de cach√© con permisos apropiados
ENV TRANSFORMERS_CACHE=/usr/local/share/transformers_cache
ENV HF_HOME=/usr/local/share/huggingface
ENV SENTENCE_TRANSFORMERS_HOME=/usr/local/share/sentence_transformers
ENV NLTK_DATA=/usr/local/share/nltk_data

# Crear directorios con permisos 755
RUN mkdir -p ${TRANSFORMERS_CACHE} ${HF_HOME} ${SENTENCE_TRANSFORMERS_HOME} ${NLTK_DATA} && \
    chmod -R 755 ${TRANSFORMERS_CACHE} ${HF_HOME} ${SENTENCE_TRANSFORMERS_HOME} ${NLTK_DATA}

# Descargar modelos en build time (como root)
RUN python -c "from sentence_transformers import SentenceTransformer; SentenceTransformer('all-MiniLM-L6-v2')"
RUN python -c "import nltk; nltk.download('stopwords', download_dir='${NLTK_DATA}', quiet=True)"
```

#### **2. app/services/utils.py - Manejo Robusto de Errores**

```python
try:
    nltk.data.find('corpora/stopwords')
except LookupError:
    try:
        nltk.download('stopwords', quiet=True)
    except Exception as e:
        # Fallback: asume que ya est√°n disponibles
        print(f"Warning: Could not download NLTK stopwords: {e}")
```

#### **3. app/services/nlp.py - Carga Desde Cach√©**

```python
cache_dir = os.environ.get('SENTENCE_TRANSFORMERS_HOME') or os.environ.get('TRANSFORMERS_CACHE')
if cache_dir:
    self.sentence_transformer = SentenceTransformer('all-MiniLM-L6-v2', cache_folder=cache_dir)
```

### **Resultado**

‚úÖ Todos los modelos se descargan en **build time** (como root)  
‚úÖ Se guardan en directorios con **permisos 755**  
‚úÖ Usuario `appuser` puede **leer** (pero no escribir) los cach√©s  
‚úÖ Aplicaci√≥n inicia sin errores de permisos

---

## üîç Troubleshooting

### **1. Error: "API key inv√°lida o faltante"**

**Problema:** Request sin header `X-API-Key` o con clave incorrecta.

**Soluci√≥n:**
```bash
# Aseg√∫rate de incluir el header correcto
curl -H "X-API-Key: your-secret-api-key-here" ...
```

### **2. Error: "Connection refused" o "Cannot connect"**

**Problema:** El servidor no est√° corriendo.

**Soluci√≥n:**
```bash
# Verificar que el servidor est√© levantado
uvicorn app.main:app --host 0.0.0.0 --port 8080

# Verificar puerto
netstat -ano | findstr :8080  # Windows
lsof -i :8080                 # Linux/Mac
```

### **3. Error: "PermissionError" en Docker**

**Problema:** Modelos NLP no pueden descargarse en runtime.

**Soluci√≥n:** Ya est√° corregido en el Dockerfile actual. Si persiste:
```bash
# Rebuild la imagen desde cero
docker build --no-cache -t seo-analysis-api .
```

### **4. Error: "Out of Memory" en Cloud Run**

**Problema:** Memoria insuficiente para modelos NLP.

**Soluci√≥n:**
```bash
# Aumentar memoria a 2GB m√≠nimo
gcloud run deploy diagnosticador-seois --memory 2Gi
```

### **5. Error: "Timeout" en an√°lisis de dominio**

**Problema:** El an√°lisis tarda m√°s que el timeout configurado.

**Soluci√≥n:**
```bash
# Aumentar timeout a 300 segundos
gcloud run deploy diagnosticador-seois --timeout 300

# O reducir max_urls en el request
{
  "domain": "https://ejemplo.com",
  "max_urls": 10  # Reducir de 50 a 10
}
```

### **6. Cloud Run muestra "placeholder"**

**Problema:** El build fall√≥ y no hay c√≥digo desplegado.

**Soluci√≥n:**
```bash
# Ver logs de Cloud Build
gcloud builds list --limit=5
gcloud builds log [BUILD_ID]

# Redesplegar
git push origin master  # Si tienes CI/CD
# O deploy directo
gcloud run deploy diagnosticador-seois --source .
```

### **7. Puerto incorrecto en Railway/Cloud Run**

**Problema:** La aplicaci√≥n no responde porque escucha en puerto incorrecto.

**Verificaci√≥n:** El archivo `app/main.py` ya est√° configurado para leer `PORT` de la variable de entorno:
```python
port = int(os.environ.get("PORT", 8080))
```

### **8. Modelos NLP no cargan**

**Problema:** Primera vez que corres la app, los modelos deben descargarse.

**Soluci√≥n:** Espera ~20-30 segundos en el primer inicio. Los modelos se descargan autom√°ticamente.

---

## üìä Especificaciones T√©cnicas

### **Recursos Necesarios**

- **RAM:** 2GB m√≠nimo (modelos NLP)
- **CPU:** 1 vCPU suficiente
- **Disco:** ~1.5GB (incluye modelos)
- **Network:** Ancho de banda normal

### **Tiempos de Respuesta**

- **Health check:** <100ms
- **An√°lisis URL individual:** 5-10 segundos
- **An√°lisis dominio (10 URLs):** 30-60 segundos
- **Startup time:** 10-15 segundos

### **Modelos NLP Incluidos**

- **SentenceTransformer:** all-MiniLM-L6-v2 (~90MB)
- **NLTK stopwords:** Spanish + English (~1MB)
- **YAKE:** Sin modelo pre-descargado (on-demand)

---

## üéØ Caracter√≠sticas Principales

### **An√°lisis Completo**

‚úÖ Extracci√≥n de metadatos (title, description, og tags)  
‚úÖ An√°lisis de headings (H1, H2, H3)  
‚úÖ Estad√≠sticas de contenido (palabras, tiempo de lectura, enlaces)  
‚úÖ Clasificaci√≥n de tipo de p√°gina (ecommerce, blog, mixto)  
‚úÖ Detecci√≥n de audiencia (B2B, B2C, gaming, professionals, etc.)  
‚úÖ Detecci√≥n de intenci√≥n (comercial, informacional, transaccional)  
‚úÖ Extracci√≥n de informaci√≥n de marca  
‚úÖ Extracci√≥n de productos (schema.org)

### **Keywords Inteligentes**

‚úÖ Extracci√≥n con YAKE + KeyBERT  
‚úÖ Fusi√≥n y deduplicaci√≥n inteligente  
‚úÖ Scoring con f√≥rmula ponderada personalizable  
‚úÖ Bucketizaci√≥n en 3 categor√≠as:
   - **Cliente:** Keywords de marca/empresa
   - **Producto/Post:** Keywords espec√≠ficas de contenido
   - **Generales SEO:** Keywords amplias y head terms

### **Procesamiento As√≠ncrono**

‚úÖ Descarga paralela de m√∫ltiples URLs  
‚úÖ Control de concurrencia (sem√°foros)  
‚úÖ Rate limiting configurable  
‚úÖ Manejo robusto de errores  
‚úÖ Timeout por request configurable

---

## üìö Documentaci√≥n Adicional

### **Documentaci√≥n Interactiva (Swagger)**

Una vez corriendo, accede a:
```
http://localhost:8080/docs
```

### **Documentaci√≥n ReDoc**

```
http://localhost:8080/redoc
```

### **Logs**

```bash
# Ver logs en tiempo real (Docker)
docker logs -f container-name

# Ver logs en Cloud Run
gcloud run logs read diagnosticador-seois --limit=50

# Ver logs en Railway
railway logs
```

---

## ‚úÖ Checklist de Deployment

- [ ] C√≥digo pusheado al repositorio
- [ ] Variables de entorno configuradas
- [ ] API_KEY cambiado a valor seguro
- [ ] Proyecto creado en Railway o Cloud Run
- [ ] Primer deployment exitoso
- [ ] Health check funcionando (`/healthz`)
- [ ] Documentaci√≥n accesible (`/docs`)
- [ ] Endpoint `/analyze-url` probado
- [ ] Endpoint `/analyze-domain` probado
- [ ] Logs verificados sin errores
- [ ] Configuraci√≥n de memoria adecuada (2GB)

---

## üéâ Estado del Proyecto

**Versi√≥n:** 1.0.0  
**Estado:** ‚úÖ **PRODUCCI√ìN READY**

- ‚úÖ Todos los errores de permisos corregidos
- ‚úÖ Docker optimizado y funcional
- ‚úÖ Configuraci√≥n Railway completa
- ‚úÖ Configuraci√≥n Cloud Run completa
- ‚úÖ Documentaci√≥n completa
- ‚úÖ Tests pasados exitosamente
- ‚úÖ Listo para deployment

---

## üìû Soporte

Para problemas o preguntas:
- **Logs de build:** Ver en Cloud Console o Railway Dashboard
- **Health check:** `curl https://tu-app/healthz`
- **Documentaci√≥n API:** `https://tu-app/docs`

---

**Desarrollado con ‚ù§Ô∏è usando FastAPI, YAKE, KeyBERT y MiniLM**
